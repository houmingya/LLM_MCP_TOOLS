"""
çŸ¥è¯†å›¾è°±å·¥å…·æ¨¡å—
æä¾›å®ä½“æŠ½å–ã€å…³ç³»æŠ½å–ã€çŸ¥è¯†å›¾è°±æ„å»ºå’ŒæŸ¥è¯¢åŠŸèƒ½
æ”¯æŒæŒä¹…åŒ–å­˜å‚¨å’Œå¢é‡æ›´æ–°
"""

import logging
import json
import pickle
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from collections import defaultdict
import networkx as nx
from openai import OpenAI
from config.settings import LLMConfig, KNOWLEDGE_GRAPH_DIR

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class KnowledgeGraphTools:
    """çŸ¥è¯†å›¾è°±å·¥å…·ç±» - æ”¯æŒæŒä¹…åŒ–å’Œå¢é‡æ›´æ–°"""
    
    def __init__(self, storage_dir: Path = KNOWLEDGE_GRAPH_DIR):
        """
        åˆå§‹åŒ–çŸ¥è¯†å›¾è°±
        
        Args:
            storage_dir: æŒä¹…åŒ–å­˜å‚¨ç›®å½•
        """
        self.storage_dir = storage_dir
        self.graph_file = storage_dir / "knowledge_graph.gpickle"  # NetworkX å›¾æ–‡ä»¶
        self.metadata_file = storage_dir / "metadata.json"  # å…ƒæ•°æ®æ–‡ä»¶
        
        # ä½¿ç”¨ NetworkX åˆ›å»ºæœ‰å‘å›¾
        self.graph = nx.DiGraph()
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ç”¨äºå®ä½“å’Œå…³ç³»æŠ½å–
        self.client = OpenAI(
            api_key=LLMConfig.API_KEY,
            base_url=LLMConfig.API_BASE
        )
        
        # å®ä½“ç±»å‹å®šä¹‰
        self.entity_types = [
            "å…¬å¸/ç»„ç»‡",
            "äººç‰©",
            "äº§å“/æœåŠ¡",
            "æŠ€æœ¯/æ¦‚å¿µ",
            "åœ°ç‚¹",
            "æ—¶é—´",
            "é¡¹ç›®/æ–¹æ¡ˆ"
        ]
        
        # å…³ç³»ç±»å‹å®šä¹‰
        self.relation_types = [
            "ä»»èŒäº",
            "ç”Ÿäº§/æä¾›",
            "ä½äº",
            "å±äº",
            "ä½¿ç”¨",
            "åˆä½œ",
            "å‚ä¸",
            "æ‹¥æœ‰",
            "å¼€å‘",
            "åº”ç”¨äº"
        ]
        
        # å°è¯•ä»ç£ç›˜åŠ è½½å·²æœ‰çš„çŸ¥è¯†å›¾è°±
        self.load_graph()
        
        logger.info(f"çŸ¥è¯†å›¾è°±å·¥å…·åˆå§‹åŒ–æˆåŠŸ - èŠ‚ç‚¹æ•°: {self.graph.number_of_nodes()}, è¾¹æ•°: {self.graph.number_of_edges()}")
    
    def save_graph(self) -> bool:
        """
        ä¿å­˜çŸ¥è¯†å›¾è°±åˆ°ç£ç›˜
        
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            # ä¿å­˜å›¾ç»“æ„ï¼ˆä½¿ç”¨ pickleï¼‰
            # æ³¨æ„ï¼šæ–°ç‰ˆ NetworkX ä¸­ write_gpickle å·²è¢«ç§»é™¤ï¼Œæ”¹ç”¨æ ‡å‡† pickle
            with open(self.graph_file, 'wb') as f:
                pickle.dump(self.graph, f, pickle.HIGHEST_PROTOCOL)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                "nodes_count": self.graph.number_of_nodes(),
                "edges_count": self.graph.number_of_edges(),
                "entity_types": self.entity_types,
                "relation_types": self.relation_types,
                "last_updated": str(Path(self.graph_file).stat().st_mtime) if self.graph_file.exists() else None
            }
            
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±å·²ä¿å­˜: {self.graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {self.graph.number_of_edges()} æ¡è¾¹")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜çŸ¥è¯†å›¾è°±å¤±è´¥: {str(e)}")
            return False
    
    def load_graph(self) -> bool:
        """
        ä»ç£ç›˜åŠ è½½çŸ¥è¯†å›¾è°±
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            if self.graph_file.exists():
                # åŠ è½½å›¾ç»“æ„
                with open(self.graph_file, 'rb') as f:
                    self.graph = pickle.load(f)
                logger.info(f"âœ… ä»ç£ç›˜åŠ è½½çŸ¥è¯†å›¾è°±: {self.graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {self.graph.number_of_edges()} æ¡è¾¹")
                return True
            else:
                logger.info("ğŸ’¡ æœªæ‰¾åˆ°å·²ä¿å­˜çš„çŸ¥è¯†å›¾è°±ï¼Œå°†åˆ›å»ºæ–°çš„ç©ºå›¾")
                return False
                
        except Exception as e:
            logger.error(f"âŒ åŠ è½½çŸ¥è¯†å›¾è°±å¤±è´¥: {str(e)}")
            self.graph = nx.DiGraph()  # é‡æ–°åˆ›å»ºç©ºå›¾
            return False
    
    def clear_graph(self) -> Dict[str, Any]:
        """
        æ¸…ç©ºçŸ¥è¯†å›¾è°±ï¼ˆæ…ç”¨ï¼ï¼‰
        
        Returns:
            æ¸…ç©ºç»“æœ
        """
        try:
            old_nodes = self.graph.number_of_nodes()
            old_edges = self.graph.number_of_edges()
            
            self.graph.clear()
            self.save_graph()
            
            return {
                "success": True,
                "message": f"çŸ¥è¯†å›¾è°±å·²æ¸…ç©ºï¼ˆåŸæœ‰ {old_nodes} ä¸ªèŠ‚ç‚¹, {old_edges} æ¡è¾¹ï¼‰"
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def extract_entities_and_relations(self, text: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨å¤§æ¨¡å‹ä»æ–‡æœ¬ä¸­æŠ½å–å®ä½“å’Œå…³ç³»
        
        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬
            
        Returns:
            åŒ…å«å®ä½“å’Œå…³ç³»çš„å­—å…¸
        """
        try:
            prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œæå–å‡ºæ‰€æœ‰çš„å®ä½“å’Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚

                    æ–‡æœ¬å†…å®¹ï¼š
                    {text}

                    è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼ˆå¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œä¸è¦æœ‰æ³¨é‡Šï¼Œä¸è¦æœ‰å¤šä½™çš„é€—å·ï¼‰ï¼š
                    {{
                        "entities": [
                            {{"name": "å®ä½“åç§°", "type": "å®ä½“ç±»å‹", "description": "ç®€çŸ­æè¿°"}}
                        ],
                        "relations": [
                            {{"source": "æºå®ä½“", "target": "ç›®æ ‡å®ä½“", "relation": "å…³ç³»ç±»å‹", "description": "å…³ç³»æè¿°"}}
                        ]
                    }}

                    å®ä½“ç±»å‹åŒ…æ‹¬ï¼š{', '.join(self.entity_types)}
                    å…³ç³»ç±»å‹åŒ…æ‹¬ï¼š{', '.join(self.relation_types)}

                    é‡è¦è§„åˆ™ï¼š
                    1. åªæå–é‡è¦çš„å®ä½“ï¼Œé¿å…æå–è¿‡äºç»†ç¢çš„ä¿¡æ¯
                    2. **ã€å…³é”®ã€‘å…³ç³»ä¸­çš„sourceå’Œtargetå¿…é¡»ä¸entitiesä¸­çš„nameå®Œå…¨ä¸€è‡´ï¼ˆåŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ã€ç©ºæ ¼ï¼‰**
                    3. **ã€å…³é”®ã€‘åœ¨æ·»åŠ relationsä¹‹å‰ï¼Œå…ˆæ£€æŸ¥sourceå’Œtargetæ˜¯å¦éƒ½åœ¨entitiesåˆ—è¡¨ä¸­å­˜åœ¨**
                    4. è¿”å›çº¯JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ ```json```æ ‡è®°
                    5. æ‰€æœ‰å­—ç¬¦ä¸²å¿…é¡»ç”¨åŒå¼•å·ï¼Œä¸è¦ç”¨å•å¼•å·
                    6. ä¸è¦åœ¨JSONä¸­æ·»åŠ æ³¨é‡Š
                    7. æœ€åä¸€ä¸ªå…ƒç´ åé¢ä¸è¦æœ‰é€—å·

                    ç¤ºä¾‹ï¼š
                    å¦‚æœentitiesä¸­æœ‰ {{"name": "AIè§†é¢‘åˆ†æç³»ç»Ÿ", ...}}
                    é‚£ä¹ˆrelationsä¸­åº”è¯¥ç”¨ {{"source": "AIè§†é¢‘åˆ†æç³»ç»Ÿ", ...}} 
                    è€Œä¸æ˜¯ {{"source": "AIåˆ†æç³»ç»Ÿ", ...}}
                    """

            response = self.client.chat.completions.create(
                model=LLMConfig.MODEL_NAME,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±æ„å»ºåŠ©æ‰‹ï¼Œæ“…é•¿ä»æ–‡æœ¬ä¸­æŠ½å–å®ä½“å’Œå…³ç³»ã€‚è¿”å›ç»“æœå¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000,
                response_format={"type": "json_object"}  # å¼ºåˆ¶è¿”å›JSONæ ¼å¼
            )
            
            result_text = response.choices[0].message.content.strip()
            logger.info(f"å¤§æ¨¡å‹åŸå§‹è¿”å›ï¼ˆå‰500å­—ç¬¦ï¼‰: {result_text[:500]}")
            
            # å°è¯•æå–JSONï¼ˆå¤„ç†å¯èƒ½çš„markdownåŒ…è£…ï¼‰
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0].strip()
            
            # æ¸…ç†å¯èƒ½çš„æ ¼å¼é—®é¢˜
            result_text = result_text.replace('\n', ' ').replace('\r', '')
            # å¤„ç†å•å¼•å·ï¼ˆæŸäº›æ¨¡å‹å¯èƒ½è¿”å›å•å¼•å·ï¼‰
            # result_text = result_text.replace("'", '"')
            
            try:
                result = json.loads(result_text)
            except json.JSONDecodeError as e:
                logger.error(f"JSONè§£æå¤±è´¥: {str(e)}")
                logger.error(f"é—®é¢˜æ–‡æœ¬: {result_text[:1000]}")
                
                # å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
                import re
                # ç§»é™¤æ³¨é‡Š
                result_text = re.sub(r'//.*?\n', '', result_text)
                result_text = re.sub(r'/\*.*?\*/', '', result_text, flags=re.DOTALL)
                
                try:
                    result = json.loads(result_text)
                    logger.info("JSONä¿®å¤æˆåŠŸ")
                except:
                    logger.error("JSONä¿®å¤å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
                    return {"entities": [], "relations": [], "error": f"JSONè§£æå¤±è´¥: {str(e)}"}
            
            logger.info(f"æå–äº† {len(result.get('entities', []))} ä¸ªå®ä½“å’Œ {len(result.get('relations', []))} ä¸ªå…³ç³»")
            return result
            
        except Exception as e:
            logger.error(f"å®ä½“å…³ç³»æŠ½å–å¤±è´¥: {str(e)}")
            return {"entities": [], "relations": [], "error": str(e)}
    
    def build_graph_from_document(self, content: str, filename: str) -> Dict[str, Any]:
        """
        ä»æ–‡æ¡£å†…å®¹æ„å»ºçŸ¥è¯†å›¾è°±
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            filename: æ–‡ä»¶å
            
        Returns:
            æ„å»ºç»“æœ
        """
        try:
            # å¦‚æœæ–‡æœ¬è¿‡é•¿ï¼Œåˆ†æ®µå¤„ç†
            max_chunk_size = 2000
            chunks = []
            
            if len(content) > max_chunk_size:
                # æŒ‰æ®µè½åˆ†å‰²
                paragraphs = content.split('\n')
                current_chunk = ""
                
                for para in paragraphs:
                    if len(current_chunk) + len(para) < max_chunk_size:
                        current_chunk += para + "\n"
                    else:
                        if current_chunk:
                            chunks.append(current_chunk)
                        current_chunk = para + "\n"
                
                if current_chunk:
                    chunks.append(current_chunk)
            else:
                chunks = [content]
            
            all_entities = []
            all_relations = []
            
            # å¤„ç†æ¯ä¸ªæ–‡æœ¬å—
            for i, chunk in enumerate(chunks):
                logger.info(f"å¤„ç†æ–‡æ¡£ {filename} çš„ç¬¬ {i+1}/{len(chunks)} å—")
                result = self.extract_entities_and_relations(chunk)
                
                all_entities.extend(result.get('entities', []))
                all_relations.extend(result.get('relations', []))
            
            # å»é‡å®ä½“ï¼ˆåŸºäºåç§°ï¼‰
            unique_entities = {}
            for entity in all_entities:
                name = entity['name']
                if name not in unique_entities:
                    unique_entities[name] = entity
                    # æ·»åŠ æ¥æºæ–‡æ¡£ä¿¡æ¯
                    unique_entities[name]['source_document'] = filename
            
            # æ·»åŠ å®ä½“åˆ°å›¾ä¸­ï¼ˆå¢é‡æ›´æ–°ï¼Œä¸è¦†ç›–å·²æœ‰å®ä½“ï¼‰
            new_entities = 0
            updated_entities = 0
            
            for entity in unique_entities.values():
                entity_name = entity['name']
                
                if entity_name in self.graph.nodes:
                    # å®ä½“å·²å­˜åœ¨ï¼Œæ›´æ–°ä¿¡æ¯ï¼ˆåˆå¹¶æè¿°ï¼Œä¿ç•™æ¥æºæ–‡æ¡£ï¼‰
                    existing_data = self.graph.nodes[entity_name]
                    
                    # åˆå¹¶æè¿°ï¼ˆå¦‚æœæ–°æè¿°æ›´è¯¦ç»†ï¼‰
                    old_desc = existing_data.get('description', '')
                    new_desc = entity.get('description', '')
                    if new_desc and (not old_desc or len(new_desc) > len(old_desc)):
                        self.graph.nodes[entity_name]['description'] = new_desc
                    
                    # åˆå¹¶æ¥æºæ–‡æ¡£ï¼ˆè¿½åŠ ï¼‰
                    old_docs = existing_data.get('source_document', '')
                    if filename not in old_docs:
                        self.graph.nodes[entity_name]['source_document'] = f"{old_docs}, {filename}" if old_docs else filename
                    
                    updated_entities += 1
                    logger.debug(f"ğŸ”„ æ›´æ–°å®ä½“: {entity_name}")
                else:
                    # æ–°å®ä½“ï¼Œæ·»åŠ åˆ°å›¾ä¸­
                    self.graph.add_node(
                        entity_name,
                        type=entity.get('type', 'Unknown'),
                        description=entity.get('description', ''),
                        source_document=filename
                    )
                    new_entities += 1
                    logger.debug(f"â• æ–°å¢å®ä½“: {entity_name}")
            
            logger.info(f"å®ä½“å¤„ç†å®Œæˆ: æ–°å¢ {new_entities} ä¸ªï¼Œæ›´æ–° {updated_entities} ä¸ªï¼Œå½“å‰å›¾ä¸­å…±æœ‰ {self.graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹")
            
            # æ·»åŠ å…³ç³»åˆ°å›¾ä¸­ï¼ˆå¢é‡æ›´æ–°ï¼Œç´¯ç§¯å…³ç³»ï¼‰
            added_relations = 0
            updated_relations = 0
            skipped_relations = []
            
            for relation in all_relations:
                source = relation.get('source')
                target = relation.get('target')
                relation_type = relation.get('relation', 'related_to')
                
                # æ£€æŸ¥å®ä½“æ˜¯å¦å­˜åœ¨
                source_exists = source in self.graph.nodes
                target_exists = target in self.graph.nodes
                
                if source_exists and target_exists:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„è¾¹
                    if self.graph.has_edge(source, target):
                        # è¾¹å·²å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦ç›¸åŒå…³ç³»ç±»å‹
                        existing_edge = self.graph.edges[source, target]
                        existing_relation = existing_edge.get('relation', '')
                        
                        if relation_type == existing_relation:
                            # ç›¸åŒå…³ç³»ï¼Œæ›´æ–°æè¿°ï¼ˆåˆå¹¶ï¼‰
                            old_desc = existing_edge.get('description', '')
                            new_desc = relation.get('description', '')
                            if new_desc and new_desc not in old_desc:
                                combined_desc = f"{old_desc}; {new_desc}" if old_desc else new_desc
                                self.graph.edges[source, target]['description'] = combined_desc
                            
                            # æ›´æ–°æ¥æºæ–‡æ¡£
                            old_docs = existing_edge.get('source_document', '')
                            if filename not in old_docs:
                                self.graph.edges[source, target]['source_document'] = f"{old_docs}, {filename}" if old_docs else filename
                            
                            updated_relations += 1
                            logger.debug(f"ğŸ”„ æ›´æ–°å…³ç³»: [{source}] --[{relation_type}]--> [{target}]")
                        else:
                            # ä¸åŒå…³ç³»ç±»å‹ï¼Œåœ¨æè¿°ä¸­è¿½åŠ æ–°å…³ç³»
                            existing_desc = existing_edge.get('description', '')
                            new_relation_desc = f"ã€{relation_type}ã€‘{relation.get('description', '')}"
                            
                            if new_relation_desc not in existing_desc:
                                combined_desc = f"{existing_desc}; {new_relation_desc}" if existing_desc else new_relation_desc
                                self.graph.edges[source, target]['description'] = combined_desc
                                self.graph.edges[source, target]['relation'] = f"{existing_relation}, {relation_type}"
                            
                            updated_relations += 1
                            logger.debug(f"ğŸ”„ è¿½åŠ å…³ç³»: [{source}] --[{relation_type}]--> [{target}]")
                    else:
                        # æ–°å…³ç³»ï¼Œç›´æ¥æ·»åŠ 
                        self.graph.add_edge(
                            source,
                            target,
                            relation=relation_type,
                            description=relation.get('description', ''),
                            source_document=filename
                        )
                        added_relations += 1
                        logger.debug(f"â• æ–°å¢å…³ç³»: [{source}] --[{relation_type}]--> [{target}]")
                else:
                    # è®°å½•ç¼ºå¤±çš„å®ä½“
                    missing = []
                    if not source_exists:
                        missing.append(f"æºå®ä½“'{source}'")
                    if not target_exists:
                        missing.append(f"ç›®æ ‡å®ä½“'{target}'")
                    
                    skipped_relations.append({
                        'relation': relation_type,
                        'source': source,
                        'target': target,
                        'missing': ', '.join(missing)
                    })
                    logger.warning(f"âš ï¸ è·³è¿‡å…³ç³» [{source}] --[{relation_type}]--> [{target}]: {', '.join(missing)} ä¸å­˜åœ¨")
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            logger.info(f"âœ… å…³ç³»å¤„ç†å®Œæˆ: æ–°å¢ {added_relations} æ¡ï¼Œæ›´æ–° {updated_relations} æ¡ï¼Œå½“å‰å›¾ä¸­å…±æœ‰ {self.graph.number_of_edges()} æ¡è¾¹")
            if skipped_relations:
                logger.warning(f"âš ï¸ è·³è¿‡äº† {len(skipped_relations)} æ¡å…³ç³»ï¼ˆå› ä¸ºå®ä½“ä¸å­˜åœ¨ï¼‰")
                # è¾“å‡ºå‰5ä¸ªè¢«è·³è¿‡çš„å…³ç³»ä½œä¸ºç¤ºä¾‹
                for i, rel in enumerate(skipped_relations[:5]):
                    logger.warning(f"  ç¤ºä¾‹ {i+1}: [{rel['source']}] --[{rel['relation']}]--> [{rel['target']}] (ç¼ºå¤±: {rel['missing']})")
            
            result = {
                "success": True,
                "filename": filename,
                "new_entities": new_entities,
                "updated_entities": updated_entities,
                "entities_count": len(unique_entities),
                "new_relations": added_relations,
                "updated_relations": updated_relations,
                "relations_count": added_relations + updated_relations,
                "skipped_relations_count": len(skipped_relations),
                "total_nodes": self.graph.number_of_nodes(),
                "total_edges": self.graph.number_of_edges(),
                "message": f"æˆåŠŸæ„å»ºçŸ¥è¯†å›¾è°±ï¼šæ–°å¢ {new_entities} ä¸ªå®ä½“ï¼Œæ›´æ–° {updated_entities} ä¸ªå®ä½“ï¼›æ–°å¢ {added_relations} æ¡å…³ç³»ï¼Œæ›´æ–° {updated_relations} æ¡å…³ç³»" + 
                          (f"ï¼ˆè·³è¿‡äº† {len(skipped_relations)} æ¡å…³ç³»ï¼Œå› ä¸ºå®ä½“ä¸å­˜åœ¨ï¼‰" if skipped_relations else "")
            }
            
            # å¦‚æœæœ‰è·³è¿‡çš„å…³ç³»ï¼Œæ·»åŠ åˆ°ç»“æœä¸­ä¾›è°ƒè¯•
            if skipped_relations:
                result["skipped_relations"] = skipped_relations[:10]  # åªè¿”å›å‰10ä¸ªç¤ºä¾‹
            
            # ğŸ’¾ ä¿å­˜åˆ°ç£ç›˜ï¼ˆæŒä¹…åŒ–ï¼‰
            save_success = self.save_graph()
            result["persisted"] = save_success
            
            logger.info(f"æ–‡æ¡£ {filename} çš„çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ{'å¹¶å·²ä¿å­˜åˆ°ç£ç›˜' if save_success else 'ï¼ˆä¿å­˜å¤±è´¥ï¼‰'}")
            return result
            
        except Exception as e:
            logger.error(f"æ„å»ºçŸ¥è¯†å›¾è°±å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "filename": filename,
                "error": str(e),
                "message": f"æ„å»ºçŸ¥è¯†å›¾è°±å¤±è´¥: {str(e)}"
            }
    
    def query_entity(self, entity_name: str) -> Dict[str, Any]:
        """
        æŸ¥è¯¢å®ä½“ä¿¡æ¯åŠå…¶å…³ç³»
        
        Args:
            entity_name: å®ä½“åç§°
            
        Returns:
            å®ä½“ä¿¡æ¯å’Œå…³ç³»
        """
        try:
            if entity_name not in self.graph.nodes:
                return {
                    "success": False,
                    "message": f"æœªæ‰¾åˆ°å®ä½“: {entity_name}"
                }
            
            # è·å–å®ä½“å±æ€§
            entity_data = self.graph.nodes[entity_name]
            
            # è·å–å‡ºè¾¹ï¼ˆè¯¥å®ä½“æŒ‡å‘å…¶ä»–å®ä½“çš„å…³ç³»ï¼‰
            outgoing = []
            for target in self.graph.successors(entity_name):
                edge_data = self.graph.edges[entity_name, target]
                outgoing.append({
                    "target": target,
                    "relation": edge_data.get('relation', 'related_to'),
                    "description": edge_data.get('description', '')
                })
            
            # è·å–å…¥è¾¹ï¼ˆå…¶ä»–å®ä½“æŒ‡å‘è¯¥å®ä½“çš„å…³ç³»ï¼‰
            incoming = []
            for source in self.graph.predecessors(entity_name):
                edge_data = self.graph.edges[source, entity_name]
                incoming.append({
                    "source": source,
                    "relation": edge_data.get('relation', 'related_to'),
                    "description": edge_data.get('description', '')
                })
            
            return {
                "success": True,
                "entity": {
                    "name": entity_name,
                    "type": entity_data.get('type', 'Unknown'),
                    "description": entity_data.get('description', ''),
                    "source_document": entity_data.get('source_document', '')
                },
                "outgoing_relations": outgoing,
                "incoming_relations": incoming,
                "relations_count": len(outgoing) + len(incoming)
            }
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å®ä½“å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def find_path(self, source: str, target: str, max_length: int = 5) -> Dict[str, Any]:
        """
        æŸ¥æ‰¾ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„å…³ç³»è·¯å¾„
        
        Args:
            source: æºå®ä½“
            target: ç›®æ ‡å®ä½“
            max_length: æœ€å¤§è·¯å¾„é•¿åº¦
            
        Returns:
            è·¯å¾„ä¿¡æ¯
        """
        try:
            if source not in self.graph.nodes:
                return {"success": False, "message": f"æœªæ‰¾åˆ°å®ä½“: {source}"}
            
            if target not in self.graph.nodes:
                return {"success": False, "message": f"æœªæ‰¾åˆ°å®ä½“: {target}"}
            
            # æŸ¥æ‰¾æœ€çŸ­è·¯å¾„
            try:
                path = nx.shortest_path(self.graph, source, target)
                
                # æ„å»ºè·¯å¾„æè¿°
                path_description = []
                for i in range(len(path) - 1):
                    edge_data = self.graph.edges[path[i], path[i+1]]
                    path_description.append({
                        "from": path[i],
                        "to": path[i+1],
                        "relation": edge_data.get('relation', 'related_to')
                    })
                
                return {
                    "success": True,
                    "path": path,
                    "path_length": len(path) - 1,
                    "path_description": path_description
                }
                
            except nx.NetworkXNoPath:
                return {
                    "success": False,
                    "message": f"{source} å’Œ {target} ä¹‹é—´æ²¡æœ‰è·¯å¾„"
                }
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾è·¯å¾„å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def get_graph_statistics(self) -> Dict[str, Any]:
        """
        è·å–çŸ¥è¯†å›¾è°±ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            # ç»Ÿè®¡å®ä½“ç±»å‹åˆ†å¸ƒ
            entity_types_count = defaultdict(int)
            for node in self.graph.nodes:
                node_type = self.graph.nodes[node].get('type', 'Unknown')
                entity_types_count[node_type] += 1
            
            # ç»Ÿè®¡å…³ç³»ç±»å‹åˆ†å¸ƒ
            relation_types_count = defaultdict(int)
            for edge in self.graph.edges:
                relation_type = self.graph.edges[edge].get('relation', 'related_to')
                relation_types_count[relation_type] += 1
            
            return {
                "success": True,
                "total_entities": self.graph.number_of_nodes(),
                "total_relations": self.graph.number_of_edges(),
                "entity_types": dict(entity_types_count),
                "relation_types": dict(relation_types_count),
                "graph_density": nx.density(self.graph)
            }
            
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def export_graph_data(self) -> Dict[str, Any]:
        """
        å¯¼å‡ºå›¾æ•°æ®ç”¨äºå‰ç«¯å¯è§†åŒ–
        
        Returns:
            å›¾æ•°æ®ï¼ˆnodeså’Œedgesæ ¼å¼ï¼‰
        """
        try:
            logger.info(f"å¼€å§‹å¯¼å‡ºå›¾æ•°æ®ï¼Œå½“å‰å›¾ä¸­æœ‰ {self.graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹ï¼Œ{self.graph.number_of_edges()} æ¡è¾¹")
            
            nodes = []
            for node in self.graph.nodes:
                node_data = self.graph.nodes[node]
                nodes.append({
                    "id": node,
                    "label": node,
                    "type": node_data.get('type', 'Unknown'),
                    "description": node_data.get('description', ''),
                    "source_document": node_data.get('source_document', '')
                })
            
            edges = []
            for edge in self.graph.edges:
                edge_data = self.graph.edges[edge]
                edges.append({
                    "source": edge[0],
                    "target": edge[1],
                    "relation": edge_data.get('relation', 'related_to'),
                    "description": edge_data.get('description', '')
                })
            
            logger.info(f"å¯¼å‡ºå®Œæˆï¼š{len(nodes)} ä¸ªèŠ‚ç‚¹ï¼Œ{len(edges)} æ¡è¾¹")
            
            return {
                "success": True,
                "nodes": nodes,
                "edges": edges,
                "nodes_count": len(nodes),
                "edges_count": len(edges)
            }
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºå›¾æ•°æ®å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def search_entities(self, keyword: str = "", entity_type: Optional[str] = None) -> Dict[str, Any]:
        """
        æœç´¢åŒ…å«å…³é”®è¯çš„å®ä½“
        
        Args:
            keyword: æœç´¢å…³é”®è¯ï¼ˆç©ºå­—ç¬¦ä¸²è¡¨ç¤ºè¿”å›æ‰€æœ‰å®ä½“ï¼‰
            entity_type: å®ä½“ç±»å‹è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŒ¹é…çš„å®ä½“åˆ—è¡¨
        """
        try:
            matched_entities = []
            
            # å¦‚æœå…³é”®è¯ä¸ºç©ºï¼Œè¿”å›æ‰€æœ‰å®ä½“ï¼ˆå¯èƒ½å¾ˆå¤šï¼Œé™åˆ¶æ•°é‡ï¼‰
            is_all = not keyword or keyword.strip() == ""
            
            for node in self.graph.nodes:
                node_data = self.graph.nodes[node]
                
                # ç±»å‹è¿‡æ»¤
                if entity_type and node_data.get('type') != entity_type:
                    continue
                
                # å…³é”®è¯åŒ¹é…ï¼ˆåç§°æˆ–æè¿°ï¼‰
                # å¦‚æœ keyword ä¸ºç©ºï¼Œåˆ™åŒ¹é…æ‰€æœ‰
                if is_all or (keyword.lower() in node.lower() or 
                    keyword.lower() in node_data.get('description', '').lower()):
                    matched_entities.append({
                        "name": node,
                        "type": node_data.get('type', 'Unknown'),
                        "description": node_data.get('description', ''),
                        "source_document": node_data.get('source_document', '')
                    })
                
                # å¦‚æœè¿”å›æ‰€æœ‰å®ä½“ï¼Œé™åˆ¶æœ€å¤šè¿”å› 100 ä¸ªï¼ˆé˜²æ­¢æ•°æ®è¿‡å¤§ï¼‰
                if is_all and len(matched_entities) >= 100:
                    logger.warning(f"å®ä½“æ•°é‡è¿‡å¤šï¼Œé™åˆ¶è¿”å›å‰ 100 ä¸ª")
                    break
            
            return {
                "success": True,
                "keyword": keyword if keyword else "å…¨éƒ¨",
                "entity_type": entity_type,
                "results": matched_entities,
                "count": len(matched_entities),
                "is_limited": is_all and len(matched_entities) >= 100
            }
            
        except Exception as e:
            logger.error(f"æœç´¢å®ä½“å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }


# åˆ›å»ºå…¨å±€å®ä¾‹
knowledge_graph_tools = KnowledgeGraphTools()
